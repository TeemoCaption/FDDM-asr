# configs/tokenizer_zhTW.yaml
# 目的：為 zh-TW（A 案：已在前處理移除括號內羅馬字）訓練 SentencePiece tokenizer 的設定

corpus:
  # 讀取由 preprocess.py 產生的 JSON 索引檔（請先跑完 preprocess.py）
  # 使用完整資料集訓練 tokenizer，確保詞彙表涵蓋所有分割
  # 語料庫設定。腳本會自動偵測使用合併檔案(train.json)或帶前綴的檔案(zh-TW_train.json)
  lang_prefix: "zh-TW"                      # 當找不到合併檔案時，用此前綴尋找 (e.g., zh-TW_train.json)
  base_path: "data/processed"               # 語料庫 JSON 檔案所在目錄
  text_field: "text"                      # 用來訓練 tokenizer 的欄位 (對應 preprocess.py 輸出)
  min_len: 1                              # 句子最小長度（過短的句子會濾掉）
  max_len: null                           # 不限制句子最大長度

tokenizer:
  type: "sentencepiece"                   # 目前支援 sentencepiece
  model_prefix: "spm_zhTW_A"              # 產出檔名前綴
  vocab_size: 8000                        # 詞彙表大小（起步 8k，可再調整）
  model_type: "bpe"                       # spm 的訓練模式：bpe | unigram | char
  character_coverage: 0.9995              # 覆蓋度（中文建議 >= 0.9995）
  input_sentence_size: 2000000            # 隨機子抽樣數（若語料太大時）
  shuffle_input_sentence: true            # 訓練前打散
  # 註：若要跑「中文字 + 羅馬字子詞（B 案）」可另外做一份 config
  extra_user_symbols: []

special_tokens:
  pad: "<pad>"
  bos: "<s>"
  eos: "</s>"
  unk: "<unk>"

output:
  dir: "data/tokenizer/zh-TW_A"           # 產物輸出資料夾
  export_vocab_json: "vocab.json"         # 另存一份可讀性較高的 json
