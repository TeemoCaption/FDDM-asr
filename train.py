# -*- coding: utf-8 -*-
"""
FDDM-asr è¨“ç·´ä¸»ç¨‹å¼ï¼ˆæœ€å°å¯è·‘éª¨æ¶ï¼‰
- å°‡ Step 2ï¼ˆAcousticEncoderï¼‰ã€Step 3ï¼ˆDecoderï¼‰ã€Step 4ï¼ˆL_fdï¼‰
  èˆ‡ä½ çš„ DiscreteDiffusionScheduler ä¸²æ¥ã€‚
- ä¾è«–æ–‡ç¬¬ 3.1ã€3.2ã€3.3 ç¯€å¯¦ä½œè¨“ç·´æµç¨‹ï¼›KL(D) ç”± scheduler æä¾›ã€‚

ä½¿ç”¨æ–¹å¼ï¼š
    python train.py --config configs/fddm_zhTW_base.yaml

ä½ éœ€è¦å…ˆæº–å‚™ï¼š
- ä½ çš„ tokenizer è¼¸å‡ºï¼ˆtargetsï¼šx0ï¼›pad_id è¦èˆ‡ config å°é½Šï¼‰
- ä½ çš„ DiscreteDiffusionSchedulerï¼Œéœ€å…·å‚™ä»¥ä¸‹ä»‹é¢ï¼ˆè«‹ä¾ä½  repo å¯¦éš›è·¯å¾‘ä¿®æ­£ importï¼‰ï¼š
    class DiscreteDiffusionScheduler:
        def __init__(self, T: int, ...):
            self.T = T
            self.betas = ...           # Î²_t
            self.alphas_cumprod = ...  # âˆ_{s=1}^t (1-Î²_s)
        def sample_q(self, x0, t):
            \"\"\"å‰å‘æ“´æ•£ï¼šçµ¦å®š x0 èˆ‡æ­¥æ•¸ tï¼Œå›å‚³ xtï¼ˆä¾ (1) q(xt|x0)ï¼‰ã€‚\"\"\"
        def kl_term(self, xt, x0, logits_x0, t):
            \"\"\"Eq.(6)ï¼šKL[q(xt-1|xt,x0) || p_theta(xt-1|xt,c)]ï¼Œ
            å…¶ä¸­ p_theta éœ€ä»¥ logits_x0ï¼ˆå° x_0 çš„åˆ†ä½ˆï¼‰æ§‹æˆ posterior q(xt-1|xt, x_hat0)ã€‚\"\"\"
        def w_t(self, t):
            \"\"\"è«–æ–‡ (13) çš„ w_t = âˆ_{s=1}^t (1-Î²_s)ã€‚\"\"\"

- è‹¥ä½ çš„æ–¹æ³•å‘½åä¸åŒï¼Œè«‹åœ¨ä¸‹æ–¹ Adapter å€å¡Šä¿®æ”¹å°æ¥ã€‚
"""
from __future__ import annotations
import os
import argparse
import random
import yaml
from dataclasses import dataclass

import torch
from torch.utils.data import Dataset, DataLoader

# ====== åŒ¯å…¥æœ¬å°ˆæ¡ˆæ¨¡çµ„ ======
from models.acoustic_encoder import AcousticEncoder
from models.denoise_decoder import DenoisingTransformerDecoder
from models.projection import SpeechProjector, TextEmbedding, TextProjector
from losses.fddm_losses import lfd_loss

# ====== é‡è¦æé†’ï¼šTokenizer ç‰¹æ®Š Token å°æ‡‰ ======
# åœ¨å°å…¥çœŸå¯¦è³‡æ–™æ™‚ï¼Œè«‹ç¢ºä¿ä»¥ä¸‹ token èˆ‡ tokenizer è¨­å®šä¸€è‡´ï¼š
# - pad_id: padding token IDï¼ˆé€šå¸¸ç‚º 0ï¼‰
# - bos_id: beginning of sequence token IDï¼ˆå¦‚æœä½¿ç”¨ï¼‰
# - eos_id: end of sequence token IDï¼ˆå¦‚æœä½¿ç”¨ï¼‰
# - unk_id: unknown token IDï¼ˆè™•ç† OOV è©å½™ï¼‰
# 
# ç¯„ä¾‹é…ç½®æª”è¨­å®šï¼š
# data:
#   pad_id: 0      # <pad> token
#   bos_id: 1      # <bos> tokenï¼ˆå¯é¸ï¼‰
#   eos_id: 2      # <eos> tokenï¼ˆå¯é¸ï¼‰
#   unk_id: 3      # <unk> token
#   vocab_size: 8000  # åŒ…å«ç‰¹æ®Š token çš„ç¸½è©å½™é‡

# === åŒ¯å…¥ Schedulerï¼ˆä¾å¯¦éš›è·¯å¾‘ä¿®æ­£ï¼‰ ===
_SCHED_IMPORT_OK = False
try:
    from fddm.sched.diffusion_scheduler import DiscreteDiffusionScheduler  # ä¿®æ­£ç‚ºå¯¦éš›æ¨¡çµ„è·¯å¾‘
    _SCHED_IMPORT_OK = True
except Exception as e:
    print(f"ç„¡æ³•åŒ¯å…¥ DiscreteDiffusionScheduler: {e}")
    pass

# ============ è³‡æ–™é›†éª¨æ¶ï¼ˆè«‹æ›æˆä½ çš„å‰è™•ç†ï¼‰ ============
class CVZhTWDataset(Dataset):
    """å¾ JSON ç´¢å¼•è¼‰å…¥çœŸå¯¦è³‡æ–™çš„ Datasetï¼š
    - å¾ data/processed/train.json ç­‰è¼‰å…¥æ¨£æœ¬
    - è¼‰å…¥å°æ‡‰çš„éŸ³é »æª”æ¡ˆå’Œæ–‡æœ¬
    - ä½¿ç”¨ tokenizer å°‡æ–‡æœ¬è½‰æ›ç‚º token IDs
    """
    def __init__(self, json_file: str, tokenizer_vocab_path: str, max_len: int, pad_id: int, bos_id: int = None, eos_id: int = None):
        super().__init__()
        import json
        import sentencepiece as spm
        import librosa
        import soundfile as sf
        
        # è¼‰å…¥ JSON è³‡æ–™
        with open(json_file, 'r', encoding='utf-8') as f:
            self.data = json.load(f)
        
        self.max_len = max_len
        self.pad_id = pad_id
        self.bos_id = bos_id
        self.eos_id = eos_id
        
        # è¼‰å…¥ tokenizer
        self.tokenizer = spm.SentencePieceProcessor()
        self.tokenizer.load(tokenizer_vocab_path)
        
        # éæ¿¾æœ‰æ•ˆæ¨£æœ¬ï¼ˆæœ‰éŸ³é »æª”æ¡ˆçš„ï¼‰
        self.valid_indices = []
        for i, item in enumerate(self.data):
            processed_path = item.get('processed_path')
            if processed_path and os.path.exists(processed_path):
                self.valid_indices.append(i)
        
        print(f"è¼‰å…¥ {len(self.valid_indices)} å€‹æœ‰æ•ˆæ¨£æœ¬å¾ {json_file}")
    
    def __len__(self):
        return len(self.valid_indices)
    
    def __getitem__(self, idx):
        data_idx = self.valid_indices[idx]
        item = self.data[data_idx]
        
        # è¼‰å…¥éŸ³é »
        import librosa
        import soundfile as sf
        wav, sr = librosa.load(item['processed_path'], sr=16000)
        wav = torch.tensor(wav, dtype=torch.float32)
        
        # è™•ç†æ–‡æœ¬
        text = item['normalized_sentence']
        tokens = self.tokenizer.encode(text)
        
        # æ·»åŠ ç‰¹æ®Š token
        if self.bos_id is not None:
            tokens = [self.bos_id] + tokens
        if self.eos_id is not None:
            tokens = tokens + [self.eos_id]
        
        # æˆªæ–·æˆ–å¡«å……
        if len(tokens) > self.max_len:
            tokens = tokens[:self.max_len]
        else:
            tokens += [self.pad_id] * (self.max_len - len(tokens))
        
        x0 = torch.tensor(tokens, dtype=torch.long)
        
# ============ CER è©•ä¼°æŒ‡æ¨™å¯¦ä½œ ============
# CER (Character Error Rate) è¨ˆç®—å‡½æ•¸
# è¨ˆç®—å­—å…ƒéŒ¯èª¤ç‡ï¼š(æ’å…¥ + åˆªé™¤ + æ›¿æ›) / ç¸½å­—å…ƒæ•¸
def calculate_cer(pred_text: str, target_text: str) -> float:
    # ç§»é™¤ç©ºç™½å’Œæ¨™é»ç¬¦è™Ÿï¼Œè½‰å°å¯«é€²è¡Œæ¯”è¼ƒ
    import re
    
    # ç°¡åŒ–æ–‡å­—é è™•ç†ï¼šç§»é™¤ç©ºç™½
    pred_text = pred_text.replace(' ', '')
    target_text = target_text.replace(' ', '')
    
    # è¨ˆç®—ç·¨è¼¯è·é›¢
    def levenshtein_distance(s1: str, s2: str) -> int:
        # ä½¿ç”¨å‹•æ…‹è¦åŠƒè¨ˆç®—æœ€å°ç·¨è¼¯è·é›¢
        m, n = len(s1), len(s2)
        dp = [[0] * (n + 1) for _ in range(m + 1)]
        
        # åˆå§‹åŒ–ç¬¬ä¸€è¡Œå’Œç¬¬ä¸€åˆ—
        for i in range(m + 1):
            dp[i][0] = i  # åˆªé™¤æ“ä½œ
        for j in range(n + 1):
            dp[0][j] = j  # æ’å…¥æ“ä½œ
        
        # å¡«å…… dp è¡¨æ ¼
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if s1[i - 1] == s2[j - 1]:
                    dp[i][j] = dp[i - 1][j - 1]  # å­—ç¬¦ç›¸åŒï¼Œç„¡éœ€æ“ä½œ
                else:
                    dp[i][j] = min(
                        dp[i - 1][j] + 1,     # åˆªé™¤
                        dp[i][j - 1] + 1,     # æ’å…¥
                        dp[i - 1][j - 1] + 1  # æ›¿æ›
                    )
        
        return dp[m][n]
    
    # è¨ˆç®—ç·¨è¼¯è·é›¢
    edit_distance = levenshtein_distance(pred_text, target_text)
    
    # è¨ˆç®— CERï¼šç·¨è¼¯è·é›¢ / çœŸå¯¦æ–‡å­—é•·åº¦
    if len(target_text) == 0:
        return 0.0 if len(pred_text) == 0 else 1.0
    
    cer = edit_distance / len(target_text)
    return cer

# å¾ logits è½‰æ›ç‚ºé æ¸¬æ–‡å­—çš„è¼”åŠ©å‡½æ•¸
def logits_to_text(logits: torch.Tensor, tokenizer, pad_id: int, bos_id: int = None, eos_id: int = None) -> str:
    # logits: [L, V] æˆ– [B, L, V]ï¼Œé€™è£¡å‡è¨­ [L, V]
    if logits.dim() == 3:
        # å¦‚æœæ˜¯ batchï¼Œå–ç¬¬ä¸€å€‹æ¨£æœ¬
        logits = logits[0]  # [L, V]
    
    # å–å¾—é æ¸¬çš„ token IDsï¼ˆå– argmaxï¼‰
    pred_ids = torch.argmax(logits, dim=-1).cpu().numpy()  # [L]
    
    # ç§»é™¤ padding token
    valid_ids = []
    for token_id in pred_ids:
        if token_id == pad_id:
            break
        valid_ids.append(token_id)
    
    # ç§»é™¤ bos å’Œ eos tokenï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if bos_id is not None and valid_ids and valid_ids[0] == bos_id:
        valid_ids = valid_ids[1:]
    if eos_id is not None and valid_ids and valid_ids[-1] == eos_id:
        valid_ids = valid_ids[:-1]
    
    # å°‡ token IDs è½‰æ›ç‚ºæ–‡å­—
    if valid_ids:
        text = tokenizer.decode(valid_ids)
    else:
        text = ''
    
    return text
class Config:
    seed: int
    data: dict
    model: dict
    diffusion: dict
    optim: dict
    lfd: dict
    log: dict

# ============ èˆ‡ Scheduler çš„ä»‹é¢å°æ¥ï¼ˆAdapterï¼‰ ============
class SchedulerAdapter:
    def __init__(self, scheduler):
        self.sch = scheduler
    def sample_q(self, x0: torch.Tensor, t: torch.Tensor) -> torch.Tensor:
        # è½‰æ› x0 ç‚º one-hot æ ¼å¼ä¸¦å‘¼å« scheduler çš„ q_sample æ–¹æ³•
        B, L = x0.shape
        vocab_size = self.sch.K
        x0_onehot = torch.zeros(B, L, vocab_size, device=x0.device)
        x0_onehot.scatter_(-1, x0.unsqueeze(-1), 1.0)  # è½‰ç‚º one-hot
        xt_prob = self.sch.q_sample(x0_onehot, t)  # å‘¼å«å¯¦éš›çš„æ–¹æ³•åç¨±
        # å¾æ©Ÿç‡åˆ†ä½ˆæ¡æ¨£å›é›¢æ•£ token
        return torch.multinomial(xt_prob.view(-1, vocab_size), 1).view(B, L)
    def kl_term(self, xt: torch.Tensor, x0: torch.Tensor, logits_x0: torch.Tensor, t: torch.Tensor, x_mask: torch.Tensor = None) -> torch.Tensor:
        # è¨ˆç®— KL divergence: KL[q(xt-1|xt,x0) || p_theta(xt-1|xt,c)]
        # æ”¹é€²ç¶­åº¦èšåˆï¼šå…ˆå° token ç¶­åšå‡å€¼ï¼Œå†å° batch åšå‡å€¼
        B, L, V = logits_x0.shape
        
        # è½‰æ›ç‚º one-hot æ ¼å¼
        xt_onehot = torch.zeros(B, L, V, device=xt.device)
        xt_onehot.scatter_(-1, xt.unsqueeze(-1), 1.0)
        
        x0_onehot = torch.zeros(B, L, V, device=x0.device)
        x0_onehot.scatter_(-1, x0.unsqueeze(-1), 1.0)
        
        # å¾ logits å¾—åˆ°é æ¸¬çš„ x0 æ©Ÿç‡åˆ†ä½ˆ
        x0hat_prob = torch.softmax(logits_x0, dim=-1)
        
        # è¨ˆç®—çœŸå¯¦å¾Œé©— q(xt-1|xt,x0)
        q_posterior = self.sch.q_posterior(xt_onehot, x0_onehot, t)
        
        # è¨ˆç®—é æ¸¬å¾Œé©— p_theta(xt-1|xt,c) â‰ˆ q(xt-1|xt,x0hat)
        p_posterior = self.sch.q_posterior(xt_onehot, x0hat_prob, t)
        
        # KL divergenceï¼Œæ”¹é€²æ•¸å€¼ç©©å®šæ€§
        eps = 1e-8
        kl_per_token = torch.sum(q_posterior * torch.log((q_posterior + eps) / (p_posterior + eps)), dim=-1)  # [B, L]
        
        # ç²¾ç´°çš„ç¶­åº¦èšåˆï¼šè€ƒæ…® mask çš„å½±éŸ¿
        if x_mask is not None:
            # åªå°æœ‰æ•ˆ token è¨ˆç®—å¹³å‡
            kl_per_sample = (kl_per_token * x_mask.float()).sum(dim=1) / (x_mask.sum(dim=1).float() + eps)  # [B]
        else:
            # å°æ‰€æœ‰ token ä½ç½®å–å‡å€¼
            kl_per_sample = kl_per_token.mean(dim=1)  # [B]
        
        # å° batch å–å‡å€¼
        return kl_per_sample.mean()  # scalar
    def w_t(self, t: torch.Tensor) -> torch.Tensor:
        # ä½¿ç”¨ scheduler çš„ alpha_bar (å³ w_prefix) å±¬æ€§
        if hasattr(self.sch, 'alpha_bar'):
            alpha_bar = self.sch.alpha_bar.to(t.device)  # [T]
            return alpha_bar[t-1]  # t æ˜¯ 1-indexed
        elif hasattr(self.sch, 'w_prefix'):
            w_prefix = self.sch.w_prefix.to(t.device)  # [T]
            return w_prefix[t-1]  # t æ˜¯ 1-indexed
        else:
            # å‚™ç”¨æ–¹æ¡ˆï¼šå¾ betas è¨ˆç®—
            if hasattr(self.sch, 'betas'):
                betas = self.sch.betas.to(t.device)  # [T]
                out = torch.ones_like(t, dtype=betas.dtype)
                for i in range(out.numel()):
                    ti = int(t.view(-1)[i].item())
                    out.view(-1)[i] = torch.prod(1.0 - betas[:ti])
                return out
            return torch.ones_like(t, dtype=torch.float32)

# ============ è¨“ç·´ä¸»æµç¨‹ ============
def train_one_epoch(
    encoder: AcousticEncoder,
    decoder: DenoisingTransformerDecoder,
    s_proj: SpeechProjector,
    t_embed: TextEmbedding,
    t_proj: TextProjector,
    scheduler: SchedulerAdapter,
    loader: DataLoader,
    optimizer: torch.optim.Optimizer,
    device: torch.device,
    cfg: Config,
    global_step: int,
) -> int:
    encoder.eval()   # é è¨­å‡çµ
    decoder.train()
    s_proj.train()
    t_embed.train()
    t_proj.train()

    pad_id = cfg.data['pad_id']
    T_total = cfg.diffusion['T']
    log_every = cfg.log['log_every']

    for batch_idx, (wave, x0) in enumerate(loader, start=1):
        wave = wave.to(device)
        x0 = x0.to(device)
        B, L = x0.shape

        # å–è²å­¸æ¢ä»¶ c = c_psi(s)
        with torch.no_grad():
            c, c_mask, _ = encoder(wave)   # [B, S, d]

        # æŠ½æ¨£æ™‚é–“æ­¥ t ~ U[1, T]
        t = torch.randint(1, T_total+1, (B,), device=device)

        # å‰å‘æ“´æ•£å¾—åˆ° x_t
        xt = scheduler.sample_q(x0, t)     # [B, L]

        # è§£ç¢¼å™¨ f_theta(xt, t, c) -> logits å° x_0 çš„åˆ†ä½ˆ
        # æ³¨æ„ï¼šx_mask æ’é™¤ padding tokenï¼Œç¢ºä¿æ¨¡å‹ä¸å­¸ç¿’é æ¸¬ padding
        logits = decoder(xt, t, c, x_mask=(x0!=pad_id), c_mask=c_mask)  # [B, L, V]

        # å»ºç«‹ token maskï¼ˆæ’é™¤ paddingï¼‰
        x_mask = (x0 != pad_id)  # [B, L] True=æœ‰æ•ˆ token
        
        # Diffusion KLï¼ˆè«–æ–‡ Eq.(6)ï¼‰
        loss_diff = scheduler.kl_term(xt, x0, logits, t, x_mask)  # å‚³å…¥ mask é€²è¡Œç²¾ç¢ºè¨ˆç®—
        
        # åˆå§‹åŒ–ç¸½æå¤±
        loss = loss_diff
        loss_fd_value = 0.0  # ç”¨æ–¼æ—¥èªŒè¨˜éŒ„
        
        # æ¯ n_step_fd æ¬¡åŠ å…¥ L_fdï¼ˆè«–æ–‡ 3.3ï¼‰
        n_step_fd = cfg.lfd['n_step_fd']
        apply_lfd = (global_step % n_step_fd) == 0
        
        if apply_lfd:
            lambda_off = cfg.lfd['lambda_offdiag']
            tau = cfg.lfd.get('tau', 1.0)
            
            # è¨ˆç®—è·¨æ¨¡æ…‹ç‰¹å¾µæŠ•å½±
            z_text = t_proj(t_embed(logits))  # [B, L, d_proj]
            z_speech = s_proj(c)             # [B, S, d_proj]
            
            # å°é½Šåºåˆ—é•·åº¦ï¼ˆèªéŸ³ç‰¹å¾µé€šå¸¸æ¯”æ–‡å­—é•·ï¼‰
            S = z_speech.size(1)
            if S >= L:
                z_speech_aligned = z_speech[:, :L, :]  # æˆªå–å‰ L å€‹æ™‚é–“æ­¥
            else:
                # é‡è¤‡æœ€å¾Œä¸€å€‹æ™‚é–“æ­¥ä¾†è£œé½Š
                pad = z_speech[:, -1:, :].repeat(1, L - S, 1)
                z_speech_aligned = torch.cat([z_speech, pad], dim=1)
            
            # è¨ˆç®—æ¬Šé‡ w_t = âˆ_{s=1}^t (1-Î²_s)
            w_t = scheduler.w_t(t).mean()  # å° batch å–å¹³å‡
            
            # è¨ˆç®— L_fd æå¤±
            loss_fd = lfd_loss(z_speech_aligned, z_text, lambda_offdiag=lambda_off)
            loss_fd_value = float(loss_fd)  # è¨˜éŒ„æ•¸å€¼ç”¨æ–¼æ—¥èªŒ
            
            # åŠ å…¥ç¸½æå¤±
            loss = loss + tau * w_t * loss_fd

        optimizer.zero_grad(set_to_none=True)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(decoder.parameters(), 5.0)
        optimizer.step()

        if (global_step % log_every) == 0:
            log_msg = f"step={global_step} loss_diff={float(loss_diff):.4f}"
            if apply_lfd:
                log_msg += f" loss_fd={loss_fd_value:.4f} w_t={float(w_t):.4f}"
            log_msg += f" total_loss={float(loss):.4f}"
            print(log_msg)
        global_step += 1

# ============ è¨“ç·´ CER è¨ˆç®—å‡½æ•¸ ============
def evaluate_train_cer(
    encoder: AcousticEncoder,
    decoder: DenoisingTransformerDecoder,
    s_proj: SpeechProjector,
    t_embed: TextEmbedding,
    t_proj: TextProjector,
    scheduler: SchedulerAdapter,
    train_loader: DataLoader,
    device: torch.device,
    cfg: Config,
    tokenizer,
    max_batches: int = 5  # åªè¨ˆç®—å‰ max_batches å€‹ batch ä¾†ç¯€çœæ™‚é–“
) -> float:
    # è¨­å®šæ¨¡å‹ç‚ºè©•ä¼°æ¨¡å¼
    encoder.eval()
    decoder.eval()
    s_proj.eval()
    t_embed.eval()
    t_proj.eval()
    
    pad_id = cfg.data['pad_id']
    total_cer = 0.0  # ç´¯è¨ˆ CER
    total_samples = 0   # ç¸½æ¨£æœ¬æ•¸
    
    with torch.no_grad():
        for batch_idx, (wave, x0) in enumerate(train_loader, start=1):
            if batch_idx > max_batches:
                break  # åªè¨ˆç®—å‰å¹¾å€‹ batch
            
            wave = wave.to(device)
            x0 = x0.to(device)
            B, L = x0.shape
            
            # å–è²å­¸æ¢ä»¶ c = c_psi(s)
            c, c_mask, _ = encoder(wave)  # [B, S, d]
            
            # åœ¨è©•ä¼°æ™‚ï¼Œä½¿ç”¨ t=1ï¼ˆæœ€å°çš„é›œè¨Šï¼‰é€²è¡Œæ¨ç†
            t = torch.ones(B, dtype=torch.long, device=device)  # [B]
            
            # å°æ–¼è©•ä¼°ï¼Œæˆ‘å€‘ä½¿ç”¨ x0 ä½œç‚º xtï¼ˆç„¡é›œè¨Šè¼¸å…¥ï¼‰
            xt = x0.clone()  # [B, L]
            
            # è§£ç¢¼å™¨æ¨ç†ï¼šf_theta(xt, t, c) -> logits å° x_0 çš„åˆ†ä½ˆ
            logits = decoder(xt, t, c, x_mask=(x0!=pad_id), c_mask=c_mask)  # [B, L, V]
            
            # å°æ¯å€‹æ¨£æœ¬è¨ˆç®— CER
            for b in range(B):
                # å–å¾—é æ¸¬æ–‡å­—
                pred_logits = logits[b]  # [L, V]
                pred_text = logits_to_text(
                    pred_logits, 
                    tokenizer, 
                    pad_id=pad_id, 
                    bos_id=cfg.data.get('bos_id'), 
                    eos_id=cfg.data.get('eos_id')
                )
                
                # å–å¾—çœŸå¯¦æ–‡å­—ï¼ˆå¾ x0 é‚„åŸï¼‰
                target_ids = x0[b].cpu().numpy()  # [L]
                valid_target_ids = []
                for token_id in target_ids:
                    if token_id == pad_id:
                        break
                    valid_target_ids.append(token_id)
                
                # ç§»é™¤ bos å’Œ eos token
                if cfg.data.get('bos_id') is not None and valid_target_ids and valid_target_ids[0] == cfg.data['bos_id']:
                    valid_target_ids = valid_target_ids[1:]
                if cfg.data.get('eos_id') is not None and valid_target_ids and valid_target_ids[-1] == cfg.data['eos_id']:
                    valid_target_ids = valid_target_ids[:-1]
                
                # å°‡ token IDs è½‰æ›ç‚ºæ–‡å­—
                if valid_target_ids:
                    target_text = tokenizer.decode(valid_target_ids)
                else:
                    target_text = ''
                
                # è¨ˆç®— CER
                cer = calculate_cer(pred_text, target_text)
                total_cer += cer
                total_samples += 1
    
    # è¨ˆç®—å¹³å‡ CER
    avg_cer = total_cer / total_samples if total_samples > 0 else 0.0
    return avg_cer

def main():
    parser.add_argument('--config', type=str, required=True)
    parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu')
    args = parser.parse_args()

    # è®€è¨­å®š
    with open(args.config, 'r', encoding='utf-8') as f:
        raw = yaml.safe_load(f)
    cfg = Config(**raw)

    random.seed(cfg.seed)
    torch.manual_seed(cfg.seed)
    torch.cuda.manual_seed_all(cfg.seed)

    device = torch.device(args.device)

    # ==== å»ºç«‹æ¨¡å‹ ====
    d_model = cfg.model['d_model']
    vocab = cfg.data['vocab_size']
    pad_id = cfg.data['pad_id']

    encoder = AcousticEncoder(**cfg.model['encoder'], d_model=d_model).to(device)
    decoder = DenoisingTransformerDecoder(
        vocab_size=vocab,
        d_model=d_model,
        nhead=cfg.model['nhead'],
        num_layers=cfg.model['num_layers'],
        dim_ff=cfg.model['dim_ff'],
        dropout=cfg.model['dropout'],
        max_len=1024,
        pad_id=pad_id,
    ).to(device)

    s_proj = SpeechProjector(d_in=d_model, d_proj=cfg.model['projector']['d_proj']).to(device)
    t_embed = TextEmbedding(vocab=vocab, d_out=cfg.model['projector']['d_proj'], mode='logits').to(device)
    t_proj  = TextProjector(d_in=cfg.model['projector']['d_proj'], d_proj=cfg.model['projector']['d_proj']).to(device)

    # ==== Scheduler ====
    if not _SCHED_IMPORT_OK:
        raise ImportError("è«‹æŠŠ DiscreteDiffusionScheduler åŠ å…¥åŒ¯å…¥è·¯å¾‘ï¼Œæˆ–ä¿®æ”¹ train.py ä¸Šæ–¹çš„ importã€‚")
    # ä½¿ç”¨æ­£ç¢ºçš„åƒæ•¸åˆå§‹åŒ– schedulerï¼šK (vocab_size), T (diffusion steps), device, beta_max
    scheduler = SchedulerAdapter(DiscreteDiffusionScheduler(
        K=vocab, 
        T=cfg.diffusion['T'], 
        device=device,
        beta_max=cfg.diffusion['beta_max']
    ))

    # ==== Optimizer ====
    params = list(decoder.parameters()) + list(s_proj.parameters()) + list(t_embed.parameters()) + list(t_proj.parameters())
    optim = torch.optim.AdamW(params, lr=cfg.optim['lr'], weight_decay=cfg.optim['weight_decay'])

    # ==== DataLoaderï¼ˆå¾ train.json, validation.json, test.json è¼‰å…¥çœŸå¯¦è³‡æ–™ï¼‰ ====
    train_json = cfg.data.get('train_json', 'data/processed/train.json')
    val_json = cfg.data.get('val_json', 'data/processed/validation.json')
    test_json = cfg.data.get('test_json', 'data/processed/test.json')
    tokenizer_model_path = cfg.data.get('tokenizer_model_path', 'data/tokenizer/zh-TW_A/spm_zhTW_A.model')
    
    # è¼‰å…¥è¨“ç·´è³‡æ–™é›†
    train_set = CVZhTWDataset(
        json_file=train_json,
        tokenizer_vocab_path=tokenizer_model_path,
        max_len=cfg.data.get('max_len', 128),
        pad_id=pad_id,
        bos_id=cfg.data.get('bos_id'),
        eos_id=cfg.data.get('eos_id')
    )
    train_loader = DataLoader(train_set, batch_size=cfg.optim['batch_size'], shuffle=True, drop_last=True)
    
    # è¼‰å…¥é©—è­‰è³‡æ–™é›†ï¼ˆå¦‚æœæª”æ¡ˆå­˜åœ¨ï¼‰
    val_loader = None
    if os.path.exists(val_json):
        val_set = CVZhTWDataset(
            json_file=val_json,
            tokenizer_vocab_path=tokenizer_model_path,
            max_len=cfg.data.get('max_len', 128),
            pad_id=pad_id,
            bos_id=cfg.data.get('bos_id'),
            eos_id=cfg.data.get('eos_id')
        )
        val_loader = DataLoader(val_set, batch_size=cfg.optim['batch_size'], shuffle=False, drop_last=False)
    
    # è¼‰å…¥æ¸¬è©¦è³‡æ–™é›†ï¼ˆå¦‚æœæª”æ¡ˆå­˜åœ¨ï¼‰
    test_loader = None
    if os.path.exists(test_json):
        test_set = CVZhTWDataset(
            json_file=test_json,
            tokenizer_vocab_path=tokenizer_model_path,
            max_len=cfg.data.get('max_len', 128),
            pad_id=pad_id,
            bos_id=cfg.data.get('bos_id'),
            eos_id=cfg.data.get('eos_id')
        )
        test_loader = DataLoader(test_set, batch_size=cfg.optim['batch_size'], shuffle=False, drop_last=False)
    
    # è¼‰å…¥ tokenizer ç”¨æ–¼ CER è¨ˆç®—
    import sentencepiece as spm
    tokenizer = spm.SentencePieceProcessor()
    tokenizer.load(tokenizer_model_path)

    # ==== è¨“ç·´ ====
    os.makedirs(cfg.log['ckpt_dir'], exist_ok=True)
    global_step = 1
    
    # åˆå§‹åŒ–æœ€ä½³æ¬Šé‡è¿½è¹¤
    best_val_cer = float('inf')  # åˆå§‹åŒ–ç‚ºç„¡é™å¤§
    best_epoch = 0
    
    for epoch in range(1, cfg.optim['num_epochs']+1):
        print(f"Epoch {epoch}")
        global_step = train_one_epoch(
            encoder, decoder, s_proj, t_embed, t_proj,
            scheduler, train_loader, optim, device, cfg, global_step
        )
        
        # æ¯å€‹ epoch çµæŸå¾Œé€²è¡Œè¨“ç·´ CER è©•ä¼°
        train_cer = evaluate_train_cer(
            encoder, decoder, s_proj, t_embed, t_proj,
            scheduler, train_loader, device, cfg, tokenizer, max_batches=5
        )
        print(f"Epoch {epoch} Train CER: {train_cer:.4f}")
        
        # æ¯å€‹ epoch çµæŸå¾Œé€²è¡Œé©—è­‰
        if val_loader is not None:
            val_cer = evaluate_cer(
                encoder, decoder, s_proj, t_embed, t_proj,
                scheduler, val_loader, device, cfg, tokenizer
            )
            print(f"Epoch {epoch} Validation CER: {val_cer:.4f}")
            
            # æª¢æŸ¥æ˜¯å¦ç‚ºæœ€ä½³é©—è­‰ CER
            if val_cer < best_val_cer:
                best_val_cer = val_cer
                best_epoch = epoch
                print(f"New best validation CER: {best_val_cer:.4f} at epoch {best_epoch}")
                
                # ä¿å­˜æœ€ä½³æ¬Šé‡
                best_ckpt = {
                    'decoder': decoder.state_dict(),
                    's_proj': s_proj.state_dict(),
                    't_embed': t_embed.state_dict(),
                    't_proj': t_proj.state_dict(),
                    'epoch': epoch,
                    'step': global_step,
                    'best_val_cer': best_val_cer,
                    'config': raw,
                }
                best_path = os.path.join(cfg.log['ckpt_dir'], 'best_model.pt')
                torch.save(best_ckpt, best_path)
                print(f"Saved best model to: {best_path}")
        
        # æ¯å€‹ epoch çµæŸå¾Œé€²è¡Œæ¸¬è©¦
        if test_loader is not None:
            test_cer = evaluate_cer(
                encoder, decoder, s_proj, t_embed, t_proj,
                scheduler, test_loader, device, cfg, tokenizer
            )
            print(f"Epoch {epoch} Test CER: {test_cer:.4f}")
        
        # å„²å­˜æ¯å€‹ epoch çš„æ¬Šé‡ï¼ˆç”¨æ–¼æ¢å¾©è¨“ç·´ï¼‰
        ckpt = {
            'decoder': decoder.state_dict(),
            's_proj': s_proj.state_dict(),
            't_embed': t_embed.state_dict(),
            't_proj': t_proj.state_dict(),
            'step': global_step,
            'epoch': epoch,
            'config': raw,
        }
        torch.save(ckpt, os.path.join(cfg.log['ckpt_dir'], f"ep{epoch:03d}.pt"))
    
    # è¨“ç·´çµæŸå¾Œå ±å‘Šæœ€ä½³æ¬Šé‡è³‡è¨Š
    print("\n" + "="*50)
    print("ğŸ† TRAINING COMPLETED!")
    print(f"Best validation CER: {best_val_cer:.4f} (Epoch {best_epoch})")
    print(f"Best model saved at: {os.path.join(cfg.log['ckpt_dir'], 'best_model.pt')}")
    print("="*50)

if __name__ == '__main__':
    main()
