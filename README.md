# FDDM-ASR å°ˆæ¡ˆ (Flow-based Discrete Diffusion Models for Automatic Speech Recognition)

åŸºæ–¼é›¢æ•£æ“´æ•£æ¨¡å‹çš„è‡ªå‹•èªéŸ³è¾¨è­˜ç³»çµ±ï¼Œæ”¯æ´ç¹é«”ä¸­æ–‡èªéŸ³è½‰æ–‡å­—ã€‚å°ˆæ¡ˆå¯¦ç¾äº† FDDM æ¶æ§‹ï¼ŒåŒ…å«ç‰¹å¾µå»ç›¸é—œæå¤± (L_fd)ã€å…ˆé€²çš„ä½ç½®ç·¨ç¢¼æŠ€è¡“ (RoPE)ã€è·¨æ¨¡æ…‹æ¢ä»¶æ§åˆ¶ (FiLM)ï¼Œä»¥åŠé«˜æ•ˆçš„è·³èºæ¡æ¨£å™¨ (Jumpy Sampler)ã€‚

## å°ˆæ¡ˆçµæ§‹

```
â”œâ”€â”€ configs/                    # æ¨¡å‹èˆ‡è¨“ç·´è¨­å®šæª”
â”‚   â”œâ”€â”€ fddm_zhTW_base.yaml     # ä¸»è¦è¨“ç·´é…ç½®ï¼ˆç¹é«”ä¸­æ–‡ï¼‰
â”‚   â”œâ”€â”€ fddm_sweep.yaml         # è¶…åƒæ•¸æœå°‹é…ç½®
â”‚   â”œâ”€â”€ tokenizer_zhTW.yaml     # Tokenizer è¨“ç·´è¨­å®š
â”‚   â””â”€â”€ diffusion.yaml          # æ“´æ•£æ¨¡å‹åƒæ•¸è¨­å®š
â”œâ”€â”€ fddm/                       # æ ¸å¿ƒæ“´æ•£æ¨¡å‹ç¨‹å¼ç¢¼
â”‚   â””â”€â”€ sched/                  # æ“´æ•£æ’ç¨‹å™¨
â”‚       â””â”€â”€ diffusion_scheduler.py  # é›¢æ•£æ“´æ•£æ’ç¨‹å™¨å¯¦ä½œ
â”œâ”€â”€ losses/                     # æå¤±å‡½æ•¸
â”‚   â””â”€â”€ fddm_losses.py          # ç‰¹å¾µå»ç›¸é—œæå¤± (L_fd)
â”œâ”€â”€ models/                     # æ¨¡å‹å®šç¾©
â”‚   â”œâ”€â”€ acoustic_encoder.py     # è²å­¸ç‰¹å¾µç·¨ç¢¼å™¨ (WavLM)
â”‚   â”œâ”€â”€ denoise_decoder.py      # å»å™ªè§£ç¢¼å™¨ (æ”¯æ´ RoPE, FiLM)
â”‚   â””â”€â”€ projection.py           # ç‰¹å¾µæŠ•å½±æ¨¡çµ„
â”œâ”€â”€ sampler/                    # æ¡æ¨£å™¨
â”‚   â””â”€â”€ jumpy_sampler.py        # è·³èºæ¡æ¨£å™¨ (æ”¯æ´ç²¾ç¢º/å¿«é€Ÿæ¨¡å¼)
â”œâ”€â”€ scripts/                    # è³‡æ–™è™•ç†èˆ‡è¨“ç·´è…³æœ¬
â”‚   â”œâ”€â”€ preprocess.py           # è³‡æ–™é è™•ç†è…³æœ¬
â”‚   â”œâ”€â”€ tokenizer_train.py      # Tokenizer è¨“ç·´è…³æœ¬
â”‚   â”œâ”€â”€ sanity_check_scheduler.py  # æ“´æ•£æ’ç¨‹å™¨æ¸¬è©¦è…³æœ¬
â”‚   â””â”€â”€ sanity_forward.py       # æ¨¡å‹å‰å‘å‚³æ’­æ¸¬è©¦è…³æœ¬
â”œâ”€â”€ .vscode/                    # VS Code è¨­å®šæª”
â”œâ”€â”€ .gitignore                 # Git å¿½ç•¥è¨­å®š
â”œâ”€â”€ README.md                  # å°ˆæ¡ˆèªªæ˜æ–‡ä»¶
â”œâ”€â”€ requirements.txt           # Python å¥—ä»¶ä¾è³´
â”œâ”€â”€ train.py                   # ä¸»è¦è¨“ç·´è…³æœ¬
â”œâ”€â”€ inference.py               # æ¨è«–è…³æœ¬
â””â”€â”€ å°ˆæ¡ˆè·¯ç·šåœ–.txt             # å°ˆæ¡ˆé–‹ç™¼è¦åŠƒèˆ‡é€²åº¦
```

## ä¸»è¦ä¾è³´å¥—ä»¶

å·²æ›´æ–°è‡³ `requirements.txt`ï¼š
- **PyTorch ç”Ÿæ…‹ç³»çµ±**: torch, torchaudio, torchvision
- **Transformers å¥—ä»¶**: transformers, sentencepiece
- **è³‡æ–™è™•ç†å¥—ä»¶**: pandas, librosa, soundfile, datasets
- **é…ç½®ç®¡ç†**: pyyaml
- **ç§‘å­¸è¨ˆç®—**: numpy, scipy

## å¿«é€Ÿé–‹å§‹

### 1. å®‰è£ä¾è³´
```bash
pip install -r requirements.txt
```

### 2. è³‡æ–™å‰è™•ç†
```bash
python scripts/preprocess.py
```
å‰è™•ç†æœƒç”¢ç”Ÿï¼š
- `data/processed/train.json`
- `data/processed/validation.json`
- `data/processed/test.json`

### 3. è¨“ç·´ Tokenizer
```bash
python scripts/tokenizer_train.py --config configs/tokenizer_zhTW.yaml
```

### 4. é–‹å§‹è¨“ç·´
```bash
python train.py --config configs/fddm_zhTW_base.yaml
```

è¨“ç·´æ™‚æœƒè‡ªå‹•ï¼š
- è¼‰å…¥ train/validation/test è³‡æ–™é›†
- æ¯å€‹ epoch çµæŸå¾Œè¨ˆç®— CER
- è‡ªå‹•ä¿å­˜æœ€ä½³é©—è­‰æ¬Šé‡
- é¡¯ç¤ºå®Œæ•´çš„è¨“ç·´æ—¥èªŒ

### 5. æ¨¡å‹æ¨è«–

#### å–®ä¸€éŸ³æª”æ¨è«–
```bash
python inference.py --wav data/processed/clips/sample.wav --ckpt ckpts/fddm_zhTW_base/best_model.pt --main-config configs/fddm_zhTW_base.yaml --diffusion-config configs/diffusion.yaml --tokenizer data/tokenizer/zh-TW_A/spm_zhTW_A.model --T-infer 20 --r 5 --greedy
```

#### æ‰¹æ¬¡æ¨è«–
```bash
python inference.py --csv data/processed/test.csv --ckpt ckpts/fddm_zhTW_base/best_model.pt --main-config configs/fddm_zhTW_base.yaml --diffusion-config configs/diffusion.yaml --tokenizer data/tokenizer/zh-TW_A/spm_zhTW_A.model --out-json results/infer_results.json
```

## è¨“ç·´èˆ‡è©•ä¼°

### æå¤±å‡½æ•¸
è¨“ç·´ä½¿ç”¨å…©ç¨®ä¸»è¦æå¤±ï¼š
1. **Diffusion KL æå¤±**: KL[q(xt-1|xt,x0) || p_theta(xt-1|xt,c)]
2. **ç‰¹å¾µå»ç›¸é—œæå¤± (L_fd)**: è«–æ–‡ 3.3 ç¯€çš„è·¨æ¨¡æ…‹å°é½Šæå¤±

### è©•ä¼°æŒ‡æ¨™
- **CER (Character Error Rate)**: å­—å…ƒéŒ¯èª¤ç‡ï¼ŒèªéŸ³è¾¨è­˜æ¨™æº–æŒ‡æ¨™
- **è¨“ç·´ CER**: ä½¿ç”¨è¨“ç·´è³‡æ–™æ¨£æœ¬è¨ˆç®—
- **é©—è­‰ CER**: æ¨¡å‹é¸æ“‡å’Œæ—©åœä¾æ“š
- **æ¸¬è©¦ CER**: æœ€çµ‚æ¨¡å‹æ€§èƒ½è©•ä¼°

### æ¬Šé‡ç®¡ç†
è¨“ç·´å®Œæˆå¾Œæœƒç”¢ç”Ÿï¼š
```
ckpts/fddm_zhTW_base/
â”œâ”€â”€ ep001.pt        # ç¬¬1å€‹epochæ¬Šé‡
â”œâ”€â”€ ep002.pt        # ç¬¬2å€‹epochæ¬Šé‡
â”œâ”€â”€ ...
â”œâ”€â”€ ep010.pt        # æœ€å¾Œä¸€å€‹epochæ¬Šé‡
â””â”€â”€ best_model.pt   # æœ€ä½³é©—è­‰CERæ¬Šé‡ï¼ˆæ¨è«–ä½¿ç”¨é€™å€‹ï¼ï¼‰
```

## è¨­å®šèªªæ˜

### fddm_zhTW_base.yaml (ä¸»è¦è¨“ç·´é…ç½®)
```yaml
# è³‡æ–™è¨­å®š
data:
  train_json: data/processed/train.json
  val_json: data/processed/validation.json
  test_json: data/processed/test.json
  vocab_size: 8000
  max_len: 128

# æ¨¡å‹è¨­å®š
model:
  d_model: 512
  nhead: 8
  num_layers: 6
  dropout: 0.1

# æ“´æ•£è¨­å®š
diffusion:
  T: 200
  beta_max: 0.01

# L_fd æå¤±è¨­å®š
lfd:
  lambda_offdiag: 5.0e-3  # å»ç›¸é—œæ‡²ç½°å¼·åº¦
  n_step_fd: 4            # æ¯4æ­¥åŠ å…¥L_fd
  tau: 1.0               # L_fdç¸½é«”ç¸®æ”¾ä¿‚æ•¸

# è¨“ç·´è¨­å®š
optim:
  lr: 2.0e-4
  batch_size: 4
  num_epochs: 10

# æ—¥èªŒè¨­å®š
log:
  ckpt_dir: ckpts/fddm_zhTW_base
  log_every: 50
```

### fddm_sweep.yaml (è¶…åƒæ•¸æœå°‹)
æ”¯æ´çš„æœå°‹åƒæ•¸ï¼š
- `lambda_offdiag`: [1e-3, 2e-3, 5e-3, 1e-2, 2e-2]
- `n_step_fd`: [1, 2, 4, 8, 16]
- `tau`: [0.1, 0.5, 1.0, 2.0, 5.0]
- `lr`: [1e-4, 2e-4, 5e-4, 1e-3]
- `batch_size`: [2, 4, 8, 16]

### æ¨è«–è¨­å®š
```yaml
inference:
  T_infer: 20          # æ¨è«–æ­¥æ•¸
  r: 2                 # è·³æ­¥é•·åº¦
  sampling_mode: "exact"  # exact/fast
  posterior_mode: "average"  # average/max
```

## è¨“ç·´è¼¸å‡ºç¯„ä¾‹

```
Epoch 1
step=50 loss_diff=2.1456 loss_fd=0.0345 w_t=0.9876 total_loss=2.1801
step=100 loss_diff=1.9876 loss_fd=0.0289 w_t=0.9765 total_loss=2.0165
...
Epoch 1 Train CER: 0.4567
Epoch 1 Validation CER: 0.4234
New best validation CER: 0.4234 at epoch 1
Saved best model to: ckpts/fddm_zhTW_base/best_model.pt
Epoch 1 Test CER: 0.4456

==================================================
TRAINING COMPLETED!
Best validation CER: 0.4234 (Epoch 1)
Best model saved at: ckpts/fddm_zhTW_base/best_model.pt
==================================================
```

## ğŸ”§ é€²éšåŠŸèƒ½

### ä½ç½®ç·¨ç¢¼é¸é …
- **RoPE**: æ—‹è½‰ä½ç½®ç·¨ç¢¼ï¼Œæ”¯æ´ä»»æ„é•·åº¦åºåˆ—
- **Sinusoidal**: æ¨™æº–æ­£å¼¦ä½ç½®ç·¨ç¢¼
- **Learned**: å¯å­¸ç¿’çš„ä½ç½®åµŒå…¥

### è·¨æ¨¡æ…‹æ¢ä»¶æ§åˆ¶
- **FiLM**: ç‰¹å¾µæ™ºæ…§ç·šæ€§èª¿è£½
- **Cross-Attention**: æ¨™æº–è·¨æ³¨æ„åŠ›æ©Ÿåˆ¶

### æ¡æ¨£å™¨æ¨¡å¼
- **ç²¾ç¢ºæ¨¡å¼ (exact)**: åš´æ ¼éµå¾ªè«–æ–‡ Algorithm 2
- **å¿«é€Ÿæ¨¡å¼ (fast)**: è¨ˆç®—æ•ˆç‡å„ªåŒ–ç‰ˆæœ¬

## æ³¨æ„äº‹é …

- **è³‡æ–™è·¯å¾‘**: ç¢ºä¿é…ç½®æª”æ¡ˆä¸­çš„è³‡æ–™è·¯å¾‘æ­£ç¢º
- **æ¬Šé‡è¼‰å…¥**: æ¨è«–æ™‚å„ªå…ˆä½¿ç”¨ `best_model.pt`
- **è¨˜æ†¶é«”ç®¡ç†**: å¤§æ¨¡å‹è¨“ç·´å»ºè­°ä½¿ç”¨ GPU
- **è¶…åƒæ•¸èª¿æ•´**: å»ºè­°å¾ç²—ç•¥ç¯„åœé–‹å§‹ï¼Œé€æ­¥ç²¾ç´°åŒ–
- **æ¨¡å‹é©—è­‰**: è¨“ç·´å‰å…ˆåŸ·è¡Œ sanity check è…³æœ¬

## ğŸ“ˆ æ€§èƒ½ç›£æ§

è¨“ç·´éç¨‹ä¸­ç›£æ§çš„é—œéµæŒ‡æ¨™ï¼š
- `loss_diff`: Diffusion KL æå¤±
- `loss_fd`: ç‰¹å¾µå»ç›¸é—œæå¤±
- `w_t`: æ™‚é–“æ¬Šé‡ä¿‚æ•¸
- `Train/Val/Test CER`: å­—å…ƒéŒ¯èª¤ç‡